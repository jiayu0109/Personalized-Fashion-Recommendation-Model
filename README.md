# Personalized-Fashion-Recommendation-Model
Using Clustering, Classification and Ranking Method for Recommendation System with H&amp;M Customer Data

### Intro
This is a group project from a Data Science course at University of Washington. (IMT 575: Data Science III: Scaling, Applications, and Ethics)
Our "H&M Personalized Fashion Recommendations" data source is from this kaggle website: https://www.kaggle.com/code/rahulappana/h-m-recommendation

### I. Data Overview
Data used in this project contains three major themes which are customer, article, and transaction. The transaction contained millions of records, so the data curation phase is difficult for us.
1. Articles: Metadata of each product available for purchase, including the product ID, product types, colors...etc. (105,542 observations)
2. Customers: Detailed information of each customer, including membership, age, postal code...etc. (31,788,324 observations)
3. Transactions: Each customer‘s transaction details, including order date, ordered products, order amount...etc. (1,371,980 observations)

We sample 300,000 observations from the Transaction table since we cannot process the full data. We’ll mention it again in our limitation section.

**The way we worked**

We utilized the Google Cloud Platform and migrated our services. Firstly, we created bucket storage to store the three CSV files. Then we used Big Query to create a cloud data warehouse based on the bucket storage. Thirdly, we configured a Compute Engine that primarily supports high CPU usage and connected with our Collab repository. Lastly, we linked the Big Query database and our Collab project so that we can have the high processing power and reduced pressure on our laptop’s RAM. 

### II. Data Curation
Below are our steps to tidy and organize our datasets in preparation for later exploratory analysis and modeling.
1. Remove missing or incomplete data; fill null values. For example, the FN, Active, and club_member_status columns from the customer dataset have missing values. We utilize two approaches: dropping the null values and replacing the null value with Scikit-learn’s SimpleImputer.
2. To ensure consistency, we correct spelling (e.g. “None” and “none”) and label categorical values using LabelEncoder.
3. Normalize numeric data by using a standard scaler
4. Extracted validation data based on each customer’s last purchase

### III. Exploratory Data Analysis
At our initial stage of exploratory data analysis, we perform some analysis exploring relationship below:
1. Purchase frequency & top 10 favorite product type
2. Purchase frequency & Fashion news familiarity
3. Top 5 Postal_code & top 10 favorite product type
4. Age group & top 10 favorite product type
In the process, we do find some potential trends between purchase frequency and purchase revenue, so we decided to do customer labeling with these two factors.

### IV. Customer Labeling
We decided to use “purchase frequency” and “purchase revenue” generated by each customer in our labeling process, also because they’re essential factors for H&M to maximize its profit. 

So we group by each customer’s **purchase counts** and **money spent** in the timeframe of our dataset 
and discretize each variable into equal-sized buckets(here, we set 1-5) based on quantiles. 
- The higher the frequency and money spent, the higher score they get. 
- With this, we define monetary score & frequency score both >= three as valuable customers

Finally, valuable customers takes 36.8% of all sampled customers

### V. Principal Component Analysis
We performed PCA (Principal Component Analysis) to reduce the curse of dimensionality, and set the variance threshold to 0.95 to get the desired component numbers. As a result, we reduced from 37 to 14 features.

### VI. K Means Clustering
We performed Kmeans clustering to differentiate customers into groups with similar shopping behaviors by using the PCA components we created. 

To choose a proper K number, we used the **Elbow method** with inertia (defined as the sum of squared distances to their closest cluster center) as the criteria. Finally, we created 10 clusters to differentiate our customers.

### VII. Logistic Regression
Referring to Wang and Han's "New Financial E-Commerce Model for Small and Medium-Sized Enterprise Financing Based on Multiple Linear Logistic Regression", we decided to perform logistic regression to predict whether the customer will purchase again in the future by using PCA components, customer groups, and the customer value tag.

The final result has an accuracy of 0.64 using cross-validation. Furthermore, by validating our prediction with the customer’s last purchase, our prediction covers 84.3% percent of the total latest purchases.

### VIII. Product Recommendation
Based on our prediction, we divided our recommendation into two categories:
(1) Customers who have purchase history and are willing to purchase again: We map each customer to a cluster and recommend **top 3 most frequently purchased items** in 7 days in each cluster. 
(2) Customers who won't purchase in the future or a new user: Use postal code and Kmean clusters to rank the top purchases.

### Limitations/Future Direction
Given the shortage of time and resource, we conclude three significant limitations of this project.
1. Disregard the time sensitivity: For future developments, we can combine time-series forecasting methods with machine learning models to make predictions with the time effect. 
2. Lack of validation for purchase tendencey: Instead of PCA, we would try forward selection model to select features since it'll provide better accuracy of recommendations and verify the effectiveness of our system.
3. Fail to provide recommendations for new accounts: It is challenging to recommend items for customers without any data. We'd like to consider top ten most popular products from H&M in the past month as one of the metrics, along with postal code.




